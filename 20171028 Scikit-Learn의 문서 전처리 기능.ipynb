{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "aecf57ef3f9b4c4481e838841b97d1d3"
   },
   "source": [
    "# Scikit-Learn의 문서 전처리 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "be175997ec674309a0ef2184e17df704"
   },
   "source": [
    "모든 데이터 분석 모형은 숫자로 구성된 고정 차원 벡터를 독립 변수로 하고 있으므로 문서(document)를 분석을 하는 경우에도 숫자로 구성된 특징 벡터(feature vector)를 문서로부터 추출하는 과정이 필요하다. 이러한 과정을 문서 전처리(document preprocessing)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e1c0b662aec34c26a468e487fb34e270"
   },
   "source": [
    "## BOW (Bag of Words)\n",
    "\n",
    "문서를 숫자 벡터로 변환하는 가장 기본적인 방법은 BOW (Bag of Words) 이다. BOW 방법에서는 전체 문서 $\\{D_1, D_2, \\ldots, D_n\\}$ 를 구성하는 고정된 단어장(vocabulary) $\\{W_1, W_2, \\ldots, W_m\\}$ 를  만들고 $D_i$라는 개별 문서에 단어장에 해당하는 단어들이 포함되어 있는지를 표시하는 방법이다.\n",
    "\n",
    "$$ \\text{ 만약 단어 } W_j \\text{가 문서} D_i \\text{ 안에 있으면 }, \\;\\; \\rightarrow x_{ij} = 1 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "c6cf0143119a4cb8accc4d518c48ebd0"
   },
   "source": [
    "## Scikit-Learn 의 문서 전처리 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "7be95ff7ab9343de904fc3a306c0c5d2"
   },
   "source": [
    "Scikit-Learn 의 feature_extraction 서브패키지와 feature_extraction.text 서브 패키지는 다음과 같은 문서 전처리용 클래스를 제공한다.\n",
    "\n",
    "* [`DictVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html):\n",
    "  * 단어의 수를 세어놓은 사전에서 BOW 벡터를 만든다.가장간단한 형태 \n",
    "* [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html): \n",
    " * 문서 집합으로부터 단어의 수를 세어 BOW 벡터를 만든다.\n",
    "* [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html): \n",
    " * 문서 집합으로부터 단어의 수를 세고 TF-IDF 방식으로 단어의 가중치를 조정한 BOW 벡터를 만든다.\n",
    "* [`HashingVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html): \n",
    " * hashing trick 을 사용하여 빠르게 BOW 벡터를 만든다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "4e98d1fa82ae4467a4cb19b3374c7537"
   },
   "source": [
    "### DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "24fcf45e4f234364ac0a3ef70d86b22d"
   },
   "source": [
    "DictVectorizer는 사전 형태로 되어 있는 feature 정보를 matrix 형태로 변환하기 위한 것으로  feature_extraction 서브 패키지에서 제공한다.\n",
    "사전 정보는  텍스트 정보에서 corpus 상의 각 단어의 사용 빈도를 나타내는 경우가 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "school_cell_uuid": "ec6fd39157174b46bcf16f6fd37bf3e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  0.],\n",
       "       [ 0.,  3.,  1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'A': 1, 'B': 2}, {'B': 3, 'C': 1}]\n",
    "X = v.fit_transform(D)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "school_cell_uuid": "d017667364754ee1a851df52d36f9a05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "school_cell_uuid": "f09e0a7072e14c85bf4551d131325fb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'A': 1.0, 'B': 2.0}, {'B': 3.0, 'C': 1.0}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "school_cell_uuid": "aa2df8c3f5fc4da2a7f8087205451069"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  4.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform({'C': 4, 'D': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "23def73151934f5bbfb54705061dacf7"
   },
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "6a9258a3bfce410e8f0e38392528ec54"
   },
   "source": [
    "`CountVectorizer`는 다양한 인수를 가진다. 그 중 중요한 것들은 다음과 같다.\n",
    "\n",
    "* `stop_words` : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    " * stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "* `analyzer` : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    " * 단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램 \n",
    "* `tokenizer` : 함수 또는 None (디폴트)\n",
    " * 토큰 생성 함수 .\n",
    "* `token_pattern` : string\n",
    " * 토큰 정의용 정규 표현식 \n",
    "* `ngram_range` : (min_n, max_n) 튜플\n",
    " * n-그램 범위 : 단어의 집합 1개가 고유명사등등 전체 단어들의 집합이 하나의 단어일때,예: 대한민국헌법\n",
    "     n-gram범위가 높을수록(잘개쪼개진 단어의 토큰집합, 단어집합 전체의 토큰 수) 결과가 좋게 나오는경우가 있다\n",
    "* `max_df` : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    " * 단어장에 포함되기 위한 최대 빈도\n",
    "* `min_df` : 정수 또는 [0.0, 1.0] 사이의 실수.  디폴트 1\n",
    " * 단어장에 포함되기 위한 최소 빈도 \n",
    "* `vocabulary` : 사전이나 리스트\n",
    " * 단어장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "school_cell_uuid": "3e709d78662e4d5aaf4222f7ec748ca8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'last': 4,\n",
       " 'one': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "school_cell_uuid": "070145846ec14965b9a119d727892539"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['This is the second document.']).toarray() ## 안에 있는 sentence를 array로 바꾸기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "school_cell_uuid": "33b0491b593c43cca2a79fab3e560f91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['Something completely new.']).toarray() ## 없는 단어를 넣어주면 처리가 안되어서 0백터를 출력, 좋은 corpus를 써야함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "school_cell_uuid": "3ec4492b57d04eb7ace34fb644734f42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "cd8d9eb7fb894be3a225a96b7b08a48d"
   },
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "674007db62ab4614a1e83af613767fc9"
   },
   "source": [
    "Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어를 말한다. 보통 영어의 관사나 접속사, 한국어의 조사 등이 여기에 해당한다. `stop_words` 인수로 조절할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "school_cell_uuid": "c19c707f5b7e4f03bd16047c41b5fe2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'first': 1, 'last': 2, 'one': 3, 'second': 4, 'third': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "school_cell_uuid": "a8f818489aed4d329b1cef167a4a5f2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\").fit(corpus) ### english는 기본적인 stop words를 없애줌 ,주로 관사, 비동사 \n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e36de7d0c9754b768e50921b97163bb0"
   },
   "source": [
    "## 토큰(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "0e5d729a6d8845daa5b7812c2653ac4e"
   },
   "source": [
    "토큰은 문서에서 단어장을 생성할 때 하나의 단어가 되는 단위를 말한다. `analyzer`, `tokenizer`, `token_pattern` 등의 인수로 조절할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "school_cell_uuid": "adad581e44824a3d99124d794d8ff6cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " '?': 2,\n",
       " 'a': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'l': 10,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " 'o': 13,\n",
       " 'r': 14,\n",
       " 's': 15,\n",
       " 't': 16,\n",
       " 'u': 17}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus) ## `analyzer`, `tokenizer`, `token_pattern` 를 직접 다 만들어줘야함 \n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "school_cell_uuid": "67bb95b158cc4e8ea86b98de62aa9276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " '?': 1,\n",
       " 'and': 2,\n",
       " 'document': 3,\n",
       " 'first': 4,\n",
       " 'is': 5,\n",
       " 'last': 6,\n",
       " 'one': 7,\n",
       " 'second': 8,\n",
       " 'the': 9,\n",
       " 'third': 10,\n",
       " 'this': 11}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "school_cell_uuid": "fb8a5de3db2a4fa5a5bcbe0c481f2c1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'third': 1, 'this': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "792d8a8be7c74d3199ee6f396e4dca8d"
   },
   "source": [
    "## n-그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "ce471098be3944c99b774b92427aa8b6"
   },
   "source": [
    "n-그램은 단어장 생성에 사용할 토큰의 크기를 결정한다. 1-그램은 토큰 하나만 단어로 사용하며 2-그램은 두 개의 연결된 토큰을 하나의 단어로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "school_cell_uuid": "16377ccb9f3f4eb48a75ec3f578ba006"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and the': 0,\n",
       " 'first document': 1,\n",
       " 'is the': 2,\n",
       " 'is this': 3,\n",
       " 'last document': 4,\n",
       " 'second document': 5,\n",
       " 'second second': 6,\n",
       " 'the first': 7,\n",
       " 'the last': 8,\n",
       " 'the second': 9,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'this is': 12,\n",
       " 'this the': 13}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "school_cell_uuid": "cfb6d9eb76cf447da36031a6d30b266e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'the third': 1, 'third': 2, 'this': 3, 'this the': 4}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "26692f9b92884fa992ac62776fecb56e"
   },
   "source": [
    "## 빈도수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "0bf059b319ec4f91b24cc57b14258524"
   },
   "source": [
    "`max_df`, `min_df` 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성할 수도 있다. 토큰의 빈도가 `max_df`로 지정한 값을 초과 하거나 `min_df`로 지정한 값보다 작은 경우에는 무시한다. 인수 값은 정수인 경우 횟수, 부동소수점인 경우 비중을 뜻한다. \n",
    "너무 자주 나오는 단어들을 뺀다(골고루 나오는애들은 분별력이 떨어짐), 너무 안나오는 단어들도 뺌(overfitting 을 막음)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "school_cell_uuid": "44a67b98b54d4eb2a85c1b513bca6fef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'document': 0, 'first': 1, 'is': 2, 'this': 3},\n",
       " {'and', 'last', 'one', 'second', 'the', 'third'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "school_cell_uuid": "e8058587c97949c8a95362af3c12893c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "65e8556f8bce4860be040ae0fffb3768"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e937920a0d87431581826d6cf30ea2a1"
   },
   "source": [
    "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법이다. \n",
    "\n",
    "\n",
    "구제적으로는 문서 $d$(document)와 단어 $t$ 에 대해 다음과 같이 계산한다.\n",
    "\n",
    "$$ \\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot \\text{idf}(d, t) $$\n",
    "\n",
    "\n",
    "여기에서\n",
    "\n",
    "* $\\text{tf}(d, t)$: 단어의 빈도수\n",
    "* $\\text{idf}(d, t)$ : inverse document frequency \n",
    " \n",
    " $$ \\text{idf}(d, t) = \\log \\dfrac{n_d}{1 + \\text{df}(t)} $$\n",
    " \n",
    "* $n_d$ : 전체 문서의 수\n",
    "* $\\text{df}(t)$:  단어 $t$를 가진 문서의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "3292542e3194487e9208ca1fb3671d4e"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "school_cell_uuid": "c4bb72733a504c558e5c61c245d62a63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.38947624,  0.55775063,  0.4629834 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941651,  0.        ,  0.4629834 ],\n",
       "       [ 0.        ,  0.24151532,  0.        ,  0.28709733,  0.        ,\n",
       "         0.        ,  0.85737594,  0.20427211,  0.        ,  0.28709733],\n",
       "       [ 0.55666851,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.55666851,  0.        ,  0.26525553,  0.55666851,  0.        ],\n",
       "       [ 0.        ,  0.38947624,  0.55775063,  0.4629834 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941651,  0.        ,  0.4629834 ],\n",
       "       [ 0.        ,  0.45333103,  0.        ,  0.        ,  0.80465933,\n",
       "         0.        ,  0.        ,  0.38342448,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv = TfidfVectorizer().fit(corpus) ## 가중치를 줬기때문에 이산변수들이 실수로 변함 \n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "071c8034b0ad4ba49879832c487592ec"
   },
   "source": [
    "## Hashing Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "ae25f3c0e0dc480496dfe0cba85b23bb"
   },
   "source": [
    "`CountVectorizer`는 모든 작업을 메모리 상에서 수행하므로 처리할 문서의 크기가 커지면 속도가 느려지거나 실행이 불가능해진다. 이 때  `HashingVectorizer`를 사용하면 해시 함수를 사용하여 단어에 대한 인덱스 번호를 생성하기 때문에 메모리 및 실행 시간을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "school_cell_uuid": "44975509dd7d41bc9a0163faa36ff4ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "school_cell_uuid": "81e28a59c9ca4747959ae4bba434b95d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.53 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time CountVectorizer().fit(twenty.data).transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "baf82c040ffd4710a455eabfb2686251"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "school_cell_uuid": "f14946419c1f482bbad18c1afe78cec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 112863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time hv.transform(twenty.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "d36e9b0bb24745a4906cf0ebeb7a50d5"
   },
   "source": [
    "## 형태소 분석기 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "school_cell_uuid": "51360cd8571a46549c68c1ce6f1ff193"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bought': 0,\n",
       " 'buying': 1,\n",
       " 'buys': 2,\n",
       " 'image': 3,\n",
       " 'imagination': 4,\n",
       " 'imagine': 5,\n",
       " 'imaging': 6}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"imaging\", \"image\", \"imagination\", \"imagine\", \"buys\", \"buying\", \"bought\"]\n",
    "vect = CountVectorizer().fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "1e34cef8c3354fbaa25e4d3312fd1803"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "docs = twenty.data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "school_cell_uuid": "31fac8608bb14d6db9d679b19d121243"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'write': 0,\n",
       " 'writer': 1,\n",
       " 'writers': 2,\n",
       " 'writes': 3,\n",
       " 'writing': 4,\n",
       " 'writing_': 5,\n",
       " 'written': 6}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\", token_pattern=\"wri\\w+\").fit(docs)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "school_cell_uuid": "fda4801e668e456baf2a6636d6afe52d",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'write': 0, 'writer': 1, 'writing_': 2, 'written': 3}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.s = SnowballStemmer('english')\n",
    "        self.t = CountVectorizer(stop_words=\"english\", token_pattern=\"wri\\w+\").build_tokenizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.s.stem(t) for t in self.t(doc)]\n",
    "\n",
    "vect = CountVectorizer(tokenizer=StemTokenizer()).fit(docs)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "04161b97391745468b05afb74dab094a"
   },
   "source": [
    "## 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "6e27091991c4418e9d5b53aa8b50244a"
   },
   "source": [
    "다음은 Scikit-Learn의 문자열 분석기를 사용하여 웹사이트에 특정한 단어가 어느 정도 사용되었는지 빈도수를 알아보는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "school_cell_uuid": "4933b282691247e0b7d38815c215e14e",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-3edacc7adbc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mhannanum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import string\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "f = urlopen(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "json = json.loads(f.read())\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == \"markdown\"]\n",
    "docs = [w for w in hannanum.nouns(\" \".join(cell)) if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "dfd1343e71854b40aa0af80066f2db8d"
   },
   "source": [
    "여기에서는 하나의 문서가 하나의 단어로만 이루어져 있다. 따라서 `CountVectorizer`로 이 문서 집합을 처리하면  각 문서는 하나의 원소만 1이고 나머지 원소는 0인 벡터가 된다. 이 벡터의 합으로 빈도를 알아보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "school_cell_uuid": "be5e453cb5374529b225bed083657781"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/5PQwVjDV8sfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b298c4c390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis=0)\n",
    "idx = np.argsort(-count)\n",
    "count = count[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "school_cell_uuid": "b03184406e9b444398f9cbc07055bbbc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(feature_name, count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
